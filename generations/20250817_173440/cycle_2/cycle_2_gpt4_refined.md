AI coding assistants like Cursor and Replit have rewritten the rules of software distribution almost overnight.

But what happens when every new user actually deepens your losses?

For decades, software distribution followed a familiar pattern: build a product, scale the user base, and watch margins improve as infrastructure costs flattened. Companies like Dropbox and Slack mastered this playbook, subsidizing growth with venture capital, betting that economies of scale would eventually deliver profits. The model depended on predictable unit economics—each new customer added revenue, but infrastructure costs scaled gently, rarely threatening the bottom line.

Now, AI has upended that logic. Instead of a flat cost curve, every prompt, inference, and model call triggers a fresh expense, often paid directly to hyperscale cloud providers. The cost to serve a single user can balloon unpredictably, especially when power users generate thousands of queries or run complex code completions. Unlike traditional SaaS, where server costs fade into the background, AI infrastructure bills can dominate the P&L—sometimes even outpacing revenue growth. The Information reported that AI now represents 67% of all new software bookings, a staggering shift in the industry’s center of gravity.

Cursor’s meteoric rise to a million users in less than a year exemplifies this new reality. Their growth echoes the viral adoption cycles of earlier SaaS successes, but the economics diverge sharply. Replit, another AI-native platform, saw its gross margins swing from 36% to -14% in one year, as some users racked up infrastructure bills in the tens of thousands of dollars. These aren’t isolated incidents. Even the largest cloud providers have flagged the unpredictable cost profile of generative AI workloads, citing the strain on GPUs and the need for constant model retraining.

The question now is whether AI companies can escape the gravity of negative margins. Some are experimenting with two-tier pricing—offering a free, limited tier to drive adoption while gating heavier usage behind paid plans. Others are investing in proprietary models or custom hardware to bend the cost curve in their favor. The challenge is that competition remains fierce, and users have little patience for paywalls or throttled features, especially when open-source alternatives exist.

There are signs of hope. Companies able to tightly align usage with value—charging per code completion, for example, or offering premium integrations—can begin to claw back margin. Some, like OpenAI, have negotiated direct infrastructure partnerships to reduce per-inference costs, while others focus on enterprise contracts where usage is more predictable and support requirements justify higher pricing. The market is also seeing the emergence of usage caps and fair-use policies, tactics borrowed from the cloud computing era to keep runaway costs in check.

Still, the fundamental question persists: can AI-native software companies grow without bleeding cash on every new user? The answer will depend on their ability to blend technical innovation with pricing discipline—balancing growth against the harsh economics of generative AI. The winners will be those who can transform spiraling infrastructure costs from a liability into a competitive moat, turning the very thing that threatens their margins into an engine for sustainable growth.